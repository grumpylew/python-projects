{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "96fba6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "39df81d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08e0294",
   "metadata": {},
   "source": [
    "#### Starting Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ab7ad0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName('Practice') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a271d6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-BVGHQDK:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Basics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x26cb15bed30>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01029a",
   "metadata": {},
   "source": [
    "#### Checking Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ac055f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b11cd7be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Price in US Dollars: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6822ce34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Price in US Dollars: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('header','true') \\\n",
    "                .option('dateFormat', 'M/d/y') \\\n",
    "                .option('inferSchema', 'true') \\\n",
    "                .csv('bigmac.csv')\n",
    "                \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0845e0",
   "metadata": {},
   "source": [
    "#### Basic Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "04800b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Country', 'Price in US Dollars']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "05ee94e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Country: string]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "77b8ed3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|  Country|Price in US Dollars|\n",
      "+---------+-------------------+\n",
      "|Argentina|               2.39|\n",
      "|Australia|               3.74|\n",
      "|   Brazil|               3.35|\n",
      "|  Britain|               4.22|\n",
      "|   Canada|               4.14|\n",
      "+---------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['Country','Price in US Dollars']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7ff27793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date', 'string'), ('Country', 'string'), ('Price in US Dollars', 'double')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e93e64e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+-------------------+\n",
      "|summary|  Date|  Country|Price in US Dollars|\n",
      "+-------+------+---------+-------------------+\n",
      "|  count|   652|      652|                652|\n",
      "|   mean|  null|     null|  3.742852760736198|\n",
      "| stddev|  null|     null| 1.2885335583427657|\n",
      "|    min|1/2010|Argentina|               0.66|\n",
      "|    max|7/2015|  Vietnam|               9.08|\n",
      "+-------+------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "844f6484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------------------+----------+\n",
      "|  Date|  Country|Price in US Dollars|Price (US)|\n",
      "+------+---------+-------------------+----------+\n",
      "|1/2016|Argentina|               2.39|      2.39|\n",
      "|1/2016|Australia|               3.74|      3.74|\n",
      "|1/2016|   Brazil|               3.35|      3.35|\n",
      "|1/2016|  Britain|               4.22|      4.22|\n",
      "|1/2016|   Canada|               4.14|      4.14|\n",
      "+------+---------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#adding columns\n",
    "df.withColumn('Price (US)', df['Price in US Dollars']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ed87e7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------------------+\n",
      "|  Date|  Country|Price in US Dollars|\n",
      "+------+---------+-------------------+\n",
      "|1/2016|Argentina|               2.39|\n",
      "|1/2016|Australia|               3.74|\n",
      "|1/2016|   Brazil|               3.35|\n",
      "|1/2016|  Britain|               4.22|\n",
      "|1/2016|   Canada|               4.14|\n",
      "+------+---------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dropping columns\n",
    "df.drop('Price (US)').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5a8f613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------------------+\n",
      "|  Date|Countries|Price in US Dollars|\n",
      "+------+---------+-------------------+\n",
      "|1/2016|Argentina|               2.39|\n",
      "|1/2016|Australia|               3.74|\n",
      "|1/2016|   Brazil|               3.35|\n",
      "|1/2016|  Britain|               4.22|\n",
      "|1/2016|   Canada|               4.14|\n",
      "+------+---------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#renaming columns\n",
    "df.withColumnRenamed('Country','Countries').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dbee92",
   "metadata": {},
   "source": [
    "#### Groupby Aggregate Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "15a66a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|    Country|               sum|\n",
      "+-----------+------------------+\n",
      "|     Norway| 84.44999999999999|\n",
      "|Switzerland|             82.53|\n",
      "|     Sweden|             71.47|\n",
      "|    Denmark|             61.59|\n",
      "|     Brazil|61.319999999999986|\n",
      "+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = df.groupBy('Country').sum()\n",
    "test.withColumnRenamed('sum(Price in US Dollars)','sum') \\\n",
    "    .sort('sum', ascending = False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ae1b25",
   "metadata": {},
   "source": [
    "### SPARK SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0a6d41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#register it as a temp sql table\n",
    "df.createOrReplaceTempView(\"my_spark_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4cd5c59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|    Country|               sum|\n",
      "+-----------+------------------+\n",
      "|     Norway| 84.44999999999999|\n",
      "|Switzerland|             82.53|\n",
      "|     Sweden|             71.47|\n",
      "|    Denmark|             61.59|\n",
      "|     Brazil|61.319999999999986|\n",
      "+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Country, SUM(`Price in US Dollars`) as sum\n",
    "FROM my_spark_table\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5127cd69",
   "metadata": {},
   "source": [
    "### Spark DataFrame Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ef6b641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "280e839f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json('Python-and-Spark-for-Big-Data-master\\Spark_DataFrames\\people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "12bbfc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6331c17f",
   "metadata": {},
   "source": [
    "### Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "02e2245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField, StringType, IntegerType, StructType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "84c0eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField('age', IntegerType(), True),\n",
    "               StructField('name', StringType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "83377362",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7bd079cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSON files cannot infer schema\n",
    "df = spark.read.json('Python-and-Spark-for-Big-Data-master\\Spark_DataFrames\\people.json' \\\n",
    "                            , schema=final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8537d67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315333ef",
   "metadata": {},
   "source": [
    "### Selecting/Filter Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "776119e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('Python-and-Spark-for-Big-Data-master/Spark_DataFrames/appl_stock.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0ce83cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1b2fdac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|   Volume|      Open|\n",
      "+---------+----------+\n",
      "|123432400|213.429998|\n",
      "|150476200|214.599998|\n",
      "|138040000|214.379993|\n",
      "+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.filter('Close < 500').select(['Open','Close']).show(3)\n",
    "\n",
    "#recommended way of filtering using python syntax\n",
    "df.filter(df['Close'] < 500).select(['Volume','Open']).show(3)\n",
    "\n",
    "# df.filter((df['Open'] > 200) & (df['Close'] < 500)).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f92a62e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get back output as a row object\n",
    "result = df.filter(df['Low'] == 197.16).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "743bf2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2010, 1, 22), Open=206.78000600000001, High=207.499996, Low=197.16, Close=197.75, Volume=220441900, Adj Close=25.620401)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9b558610",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cab96f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': datetime.date(2010, 1, 22),\n",
       " 'Open': 206.78000600000001,\n",
       " 'High': 207.499996,\n",
       " 'Low': 197.16,\n",
       " 'Close': 197.75,\n",
       " 'Volume': 220441900,\n",
       " 'Adj Close': 25.620401}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "78a0dff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220441900"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.asDict()['Volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc6b5f2",
   "metadata": {},
   "source": [
    "### Group By and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bea2c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('aggs').getOrCreate()\n",
    "df = spark.read.csv('Python-and-Spark-for-Big-Data-master/Spark_DataFrames/sales_info.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e2ad1dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "+-------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "413f1acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sum, count, etc...\n",
    "df.groupBy('Company').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "98ea86f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(Sales)|\n",
      "+----------+\n",
      "|    4327.0|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|max(Sales)|\n",
      "+----------+\n",
      "|     870.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#agg method\n",
    "df.agg({'Sales':'sum'}).show()\n",
    "df.agg({'Sales':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "84c4070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc34ddb3",
   "metadata": {},
   "source": [
    "### Alias and Formating Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "813db1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT Sales)|\n",
      "+---------------------+\n",
      "|                   11|\n",
      "+---------------------+\n",
      "\n",
      "+-----------------+\n",
      "|    Average sales|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct('Sales')).show()\n",
    "df.select(avg('Sales').alias('Average sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d0b9a20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Average sales|\n",
      "+-------------+\n",
      "|       360.58|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_sales = df.select(avg('Sales').alias('Average sales'))\n",
    "avg_sales.select(format_number('Average sales',2).alias('Average sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ffa87852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('Sales').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e0deeb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df['Sales'].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234af60",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0bf65c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('miss').getOrCreate()\n",
    "df = spark.read.csv('Python-and-Spark-for-Big-Data-master/Spark_DataFrames/ContainsNull.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7e9cf15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7dd56679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop if there is 2 or more null values in the row\n",
    "df.na.drop(how = 'any',thresh =2).show(5) \n",
    "\n",
    "#drop if the row is entirely null\n",
    "df.na.drop(how = 'all').show(5)\n",
    "\n",
    "#drop if there is 1 or more null values in the column\n",
    "df.na.drop(how = 'any', subset = ['Sales']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1ea73f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| null|  0.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fill missing values\n",
    "df.na.fill(0, subset = ['Sales']).show(5)\n",
    "\n",
    "#fill null sales column with mean sales\n",
    "df.na.fill(df.select(mean(df['sales'])).collect()[0][0], subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "668f5f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| null|\n",
      "|emp2|No Name| null|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('No Name', subset = ['Name']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7bab00",
   "metadata": {},
   "source": [
    "### Dates and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "37799940",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('miss').getOrCreate()\n",
    "df = spark.read.csv('Python-and-Spark-for-Big-Data-master/Spark_DataFrames/appl_stock.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6b049a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a5bc372c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      Date|      Open|\n",
      "+----------+----------+\n",
      "|2010-01-04|213.429998|\n",
      "|2010-01-05|214.599998|\n",
      "|2010-01-06|214.379993|\n",
      "+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['Date','Open']).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d2e7ca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|dayofmonth(Date)|\n",
      "+----------------+\n",
      "|               4|\n",
      "|               5|\n",
      "|               6|\n",
      "|               7|\n",
      "|               8|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+\n",
      "|hour(Date)|\n",
      "+----------+\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+\n",
      "|month(Date)|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dayofmonth\n",
    "df.select(dayofmonth(df['Date'])).show(5)\n",
    "\n",
    "#hour\n",
    "df.select(hour(df['Date'])).show(5)  \n",
    "\n",
    "#month\n",
    "df.select(month(df['Date'])).show(5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "551aadea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+\n",
      "|Year|Average Closing Price|\n",
      "+----+---------------------+\n",
      "|2010|               259.84|\n",
      "|2011|               364.00|\n",
      "|2012|               576.05|\n",
      "|2013|               472.63|\n",
      "|2014|               295.40|\n",
      "|2015|               120.04|\n",
      "|2016|               104.60|\n",
      "+----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.select(year(df['Date'])).show()\n",
    "\n",
    "#average closing price per year ordered by year\n",
    "\n",
    "#add new column, extract Year\n",
    "newdf = df.withColumn('Year',year(df['Date']))  \n",
    "\n",
    "#group by Year, get the average closing price, ordered by year\n",
    "result = newdf.groupBy('year').mean().select(['Year','avg(Close)']).orderBy('Year')\n",
    "\n",
    "#format number, round to 2dp | use alias to rename\n",
    "result = result.select(['Year',format_number('avg(close)',2).alias('Average Closing Price')])\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbe170f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Machine Learning with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f88405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "# StringIndexer is used to convert categorical string values into numerical indices.  eg female - 1, male - 0\n",
    "indexer = StringIndexer(\n",
    "    inputCols=[\"Gender\", \"Smoker\"],\n",
    "    outputCols=[\"GenderIndex\", \"SmokerIndex\"],\n",
    ")\n",
    "df_r = indexer.fit(df).transform(df)\n",
    "df_r.show()\n",
    "\n",
    "# OneHotEncoder is used to convert the indexed values into one-hot encoded vectors, ensuring that the algorithm treats each category as distinct and non-ordinal.\n",
    "gender_encoder = OneHotEncoder(inputCol=\"GenderIndex\", outputCol=\"GenderVec\")\n",
    "\n",
    "\n",
    "# Combine columns [Age, Experience] together ---> Independent Feature\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=[\"Age\", \"GenderVec\"], outputCol=\"Independent Feature\"\n",
    ")\n",
    "output = feature_assembler.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose column to predict eg. Salary with the independent column\n",
    "finalized_data = output.select(\"Independent Feature\", \"Salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ad388",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression \n",
    "\n",
    "# train test split\n",
    "train, test = finalized_data.randomSplit([0.75,0.25])\n",
    "\n",
    "# train data\n",
    "regressor = LinearRegression(featuresCol=\"Independent Features\", labelCol=\"Salary\")\n",
    "regressor = regressor.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd24ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercepts\n",
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a4f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction for test data\n",
    "pred_results = regressor.evaluate(test)\n",
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e24ae3",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5de452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf3ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba5ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24eca23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d15d4562",
   "metadata": {},
   "source": [
    "### Decision Trees, Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea908ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f54e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650627e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5734243c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fdf646e",
   "metadata": {},
   "source": [
    "### K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c631ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec6c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa04e447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c02bb20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d534e92",
   "metadata": {},
   "source": [
    "### Collaborative filtering for Recommender Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc2e902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff72712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800499c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3cb87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56c7c124",
   "metadata": {},
   "source": [
    "### Natural Language Processing NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6df260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a6db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d1558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd35a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
